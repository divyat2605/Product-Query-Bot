# -*- coding: utf-8 -*-
"""Internship_task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZzmvSxT6O0o5Jzb20waBbqR-7yLm7dl
"""

# Install the missing package
!pip install pyngrok

# Now you can import the module
from pyngrok import ngrok
ngrok.kill()  # Ends all existing tunnels

from pyngrok import ngrok

ngrok.kill()  # Kill all existing ngrok processes

!killall ngrok

# -------------------- 1. Install Dependencies --------------------
!pip install sentence-transformers faiss-cpu google-generativeai fastapi uvicorn pyngrok nest_asyncio -q

# -------------------- 2. Imports and Configuration --------------------
import os
import json
import logging
import numpy as np
import faiss
import google.generativeai as genai
import nest_asyncio
import asyncio

from sentence_transformers import SentenceTransformer
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from pyngrok import ngrok, conf

# Apply for Colab compatibility
nest_asyncio.apply()

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# -------------------- 3. Set API Keys Here --------------------
GOOGLE_API_KEY = "AIzaSyDj6ZQPWyQi_9zXxsnm1znD2gSt-7pITRM"
NGROK_AUTHTOKEN = "2vm22nydRkT7izlXdkyHsetA74J_6LDrEbLFmPHg7S1yeZMH4"

if not GOOGLE_API_KEY or not NGROK_AUTHTOKEN:
    raise RuntimeError("‚ùå Please set your GOOGLE_API_KEY and NGROK_AUTHTOKEN above!")

genai.configure(api_key=GOOGLE_API_KEY)

# -------------------- 4. Sample Data --------------------
products = [
    {
        "product_id": "P101",
        "name": "EcoBottle Max",
        "description": "Reusable smart water bottle that tracks your hydration and glows as a reminder.",
        "features": "Smart glow reminder, BPA-free, Temperature display",
        "price": "$35"
    },
    {
        "product_id": "P102",
        "name": "FlexiChair Pro",
        "description": "Ergonomic chair with lumbar support and adjustable height for remote workers.",
        "features": "Adjustable height, Lumbar support, Breathable mesh",
        "price": "$120"
    },
    {
        "product_id": "P103",
        "name": "ZenTunes Speaker",
        "description": "Bluetooth speaker with ambient LED lights and 360-degree sound.",
        "features": "360¬∞ sound, LED lights, 10-hour battery life",
        "price": "$65"
    },
    {
        "product_id": "P104",
        "name": "AirPure Mini",
        "description": "Portable air purifier perfect for small rooms and cars.",
        "features": "HEPA filter, USB charging, Low noise",
        "price": "$49"
    },
]

# -------------------- 5. Chunking and Preprocessing --------------------
def chunk_text(text, chunk_size=200, overlap=30):
    words = text.split()
    chunks, start = [], 0
    while start < len(words):
        end = start + chunk_size
        chunks.append(" ".join(words[start:end]))
        start += chunk_size - overlap
    return chunks

def preprocess_data(products):
    all_chunks, metadata = [], []
    for product in products:
        full_text = f"Product Name: {product['name']}\nDescription: {product['description']}\nFeatures: {product['features']}"
        chunks = chunk_text(full_text)
        for chunk in chunks:
            all_chunks.append(chunk)
            metadata.append({'product_id': product['product_id'], 'text': chunk})
    return all_chunks, metadata

# -------------------- 6. Embedding and Indexing --------------------
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

def generate_embeddings(chunks):
    return embedding_model.encode(chunks, show_progress_bar=True).astype('float32')

def build_faiss_index(embeddings):
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index

# -------------------- 7. Search Function --------------------
def search_index(query, index, metadata, k=3):
    query_embedding = embedding_model.encode([query]).astype('float32')
    distances, indices = index.search(query_embedding, k)
    return [metadata[i] for i in indices[0] if i < len(metadata)]

# -------------------- 8. LLM and Context --------------------
llm = genai.GenerativeModel("gemini-1.5-pro")

def format_context(retrieved_chunks):
    context_str = ""
    seen_pids = set()
    for i, chunk_info in enumerate(retrieved_chunks):
        context_str += f"Context {i+1} (Product ID: {chunk_info['product_id']}):\n{chunk_info['text']}\n\n"
        seen_pids.add(chunk_info['product_id'])
    return context_str, list(seen_pids)

async def get_llm_answer(query, context_str):
    prompt = f"""You are a helpful assistant answering product-related questions using only the context below.

Context:
{context_str}

User Question: {query}

Instructions:
- Use ONLY the information from the context.
- Mention the product ID(s) you used.
- If the answer is not available, say: "Based on the provided context, I cannot answer this question."

Answer:"""
    try:
        response = await asyncio.to_thread(llm.generate_content, prompt)
        return response.text
    except Exception as e:
        return f"‚ùå LLM error: {str(e)}"

# -------------------- 9. Precompute Embeddings --------------------
chunks, metadata = preprocess_data(products)
embeddings = generate_embeddings(chunks)
faiss_index = build_faiss_index(embeddings)

# -------------------- 10. FastAPI Setup --------------------
app = FastAPI()

class QueryRequest(BaseModel):
    query: str

@app.post("/query")
async def query_products(request: QueryRequest):
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="Query cannot be empty.")

    retrieved = search_index(request.query, faiss_index, metadata)
    context_str, pids = format_context(retrieved)
    answer = await get_llm_answer(request.query, context_str)

    return {"answer": answer, "product_ids": pids}

# -------------------- 11. Ngrok Tunnel and Uvicorn --------------------
conf.get_default().auth_token = NGROK_AUTHTOKEN
ngrok.kill()
public_url = ngrok.connect(8000)
print("üîó Public URL (use this to test the API):", public_url)

import uvicorn
import threading

def start_uvicorn():
    uvicorn.run(app, host="0.0.0.0", port=8000)

threading.Thread(target=start_uvicorn).start()

!curl -X POST https://83a7-34-23-182-130.ngrok-free.app/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Which product has LED lights?"}'